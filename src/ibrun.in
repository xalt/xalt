#!/bin/bash
# -*-sh-*-
# ---------------------------------------------------------------------
# Stampede Version (SLURM)
# ---------------------------------------------------------------------
# set -x

# Figure out which system we are on.

ibrunDir=/usr/local/bin

nlocal=$(hostname -f)
nA=($(builtin echo "$nlocal" | tr '.' ' '))
first=${nA[0]}
SYSHOST=${nA[1]}

fqdn="$SYSHOST.tacc.utexas.edu"

ADMIN_stampede="/tmp/moduleData/reverseMapD:/home1/moduleData/XSEDE/reverseMapD"
ADMIN_ls4="/home1/moduleData/reverseMapD"
ADMIN_longhorn="/share/tacc_admin/reverseMapD"

CPN_stampede=16
CPN_ls4=12
CPN_longhorn=8

eval "CPN=\$CPN_$SYSHOST"

NUMCORES=$((CPN*2))

# ------------------------------
# Default Environment Settings
# ------------------------------

# 12/16/12 (ks): Temporary fix to disable limic
# 02/26/13 (ks): limic re-enabled by default based on bug fix from OSU

#if [ -z "$MV2_SMP_USE_LIMIC2" ]; then
#    export MV2_SMP_USE_LIMIC2=0
#fi

# 01/14/13 (ks): Temporary fix to disable UD hybrid

if [ -z "$MV2_USE_UD_HYBRID" ]; then
    export MV2_USE_UD_HYBRID=0
fi

if [ -z "$MV2_USE_OLD_BCAST" ]; then
    export MV2_USE_OLD_BCAST=1
fi

# 4/16/13 - faster startup times for 1.9b and higher

if [ -z "$MV2_HOMOGENEOUS_CLUSTER" ];then
    export MV2_HOMOGENEOUS_CLUSTER=1
fi

# 4/16/13 - support for more than 128 MICs with impi

if [ -z "$I_MPI_HYDRA_BRANCH_COUNT" ]; then
 export I_MPI_HYDRA_BRANCH_COUNT=6000
fi

# 7/3/13 (ks) - disable huge pages to avoid WRF issues with MV2

if [ -z "$MV2_USE_HUGEPAGES" ]; then
    export MV2_USE_HUGEPAGES=0
fi



## Look through all the arguments to ibrun.  If the user asked for help (-h)
## print the help and then exit immediately.

if [ "$1" == "-h" -o "$1" == "--help" -o "$1" == "-help" ]; then
  ## User asked for a help message.  Exit after printing it.
    echo "Basic Usage: "
    echo "$0 ./executable <execuable_opions>"
    echo ""
    echo "In normal usage, pass only the MPI executable name and any"
    echo "options the executable requires."
    echo ""
    echo ""
    echo "Advanced Usage:"
    echo "$0 -n <number of processors> -o <offset into processor hostlist> executable <execuable_opions>"
    echo ""
    echo "In this case you can specify a subset of processors from the"
    echo "list of all hosts on which to run the executable, and an offset into the list"
    echo "of processors to start numbering.  This allows you to e.g. launch two different"
    echo "exectuables on two different subsets of the available hosts."
    echo ""
    echo "Example, parallel environment -pe ${CPN}way $NUMCORES, executable 'mpihello'"
    echo " We can do:"
    echo " $0 -n $CPN -o 0  ./mpihello &"
    echo " $0 -n $CPN -o $CPN ./mpihello &"
    echo " wait"
    echo ""
    echo "The first call launches a ${CPN}-processor job on the first $CPN hosts in the hostfile,"
    echo "The second call launches a ${CPN}-processor job on the second $CPN hosts in the hostfile."
    echo "The shell 'wait' command waits for all processes to finish before the shell exits."
    exit 0
fi

pe_startTime=`date +%s`

# Get the PE hostfile, number of slots and wayness from the environment

SCHEDULER=SLURM

if [ "$SCHEDULER" == "SGE" ];then
    pe_hostfile=$PE_HOSTFILE
    pe_slots=$NSLOTS
    pe_numNodes=`wc -l $PE_HOSTFILE`
    home_batch_dir="$HOME/.sge"
    BATCH_JOB_ID=$JOB_ID
    NSLOTS_BATCH=$NSLOTS

    # Cut out the "way" string and just get the wayness number.
    pe_ppn=`echo $PE | sed -e 's/way//g;'`

elif [ "$SCHEDULER" == "SLURM" ];then
    BATCH_JOB_ID=$SLURM_JOB_ID
    NSLOTS_BATCH=$SLURM_NPROCS
    #Parse the SLURM_TASKS_PER_NODE string to get # of node clusters
    # e.g.  6(2x),5(2x) -- 1st 2 nodes run 6 tasks, next 2 nodes run 5 tasks
    declare -a node_clusters=(`echo $SLURM_TASKS_PER_NODE | sed -e's/,/ /g'`)
#    echo "DEBUG: ${node_clusters[@]} "
    #Set the wayness for each node cluster using one env
    # Format of node_tasks_ppn_info = 
    # "{# of tasks per node},{#initial task id}_[repeats if necessary]"
    #                                         ^
    #No spaces are allowed in env variables that build_env.pl handles.
    # So, an "_" is used in place of a space.
    node_tasks_ppn_info=""
    task_count=0
    for nodes in ${node_clusters[@]}; do
      tasks_ppn_cluster=`echo $nodes | awk -F '(' '{print $1}'`
      node_tasks_ppn_info="${node_tasks_ppn_info}${tasks_ppn_cluster}"
      if [[ `echo $nodes | grep x` ]]; then
        node_count=`echo $nodes | sed -e's/.*x\([0-9]\+\).*/\1/'`
      else
       node_count=1
      fi
      node_tasks_ppn_info="${node_tasks_ppn_info},${task_count}_"
      let "total_tasks_per_node_cluster = $node_count * $tasks_ppn_cluster"
      let "task_count = $task_count + $total_tasks_per_node_cluster "
    done
    export NODE_TASKS_PPN_INFO="\"$node_tasks_ppn_info\""
#    echo "DEBUG: NODE_TASKS_PPN_INFO = $NODE_TASKS_PPN_INFO"
else
    echo "ERROR: Unknown batch system"
    exit 1
fi
# echo "DEBUG:  ppn = ${pe_ppn} "

echo "TACC: Starting up job $BATCH_JOB_ID"

srq_size=2048
default_time_out=23

# Find out which MPI stack we're using

MODE=$TACC_MPI_GETMODE
if [ -z $MODE ]; then
    MODE=`getmode.sh`
fi 

# Set our files and directories

home_batch_dir="$HOME/.slurm"

if [ ! -d $home_batch_dir ]; then
    mkdir -p $home_batch_dir
fi

# FYI: mktemp generates a filename with random numbers/letters
# replacing the XXX

hostfile_tacc=`mktemp $home_batch_dir/job.$BATCH_JOB_ID.hostlist.XXXXXXXX`
nslotsfile_tacc=`mktemp $home_batch_dir/job.$BATCH_JOB_ID.info.XXXXXXXX`

# Just to be sure, remove the host and nslots files in case they
# already exist.  This should never happen...

if [ -f $nslotsfile_tacc ]; then
    rm $nslotsfile_tacc
fi

if [ -f $hostfile_tacc ]; then
    rm $hostfile_tacc
fi

# Set local LD_PRELOAD with DARSHAN
if [ x$LD_PRELOAD == "x" ]; then
    IBRUN_LD_PRELOAD=""
else
    IBRUN_LD_PRELOAD="$LD_PRELOAD"
    unset LD_PRELOAD
fi

if [ x$TACC_DARSHAN_LIB != "x" ]; then
    if [ x$IBRUN_LD_PRELOAD == "x" ]; then
	IBRUN_LD_PRELOAD="${TACC_DARSHAN_FORTRAN_LIB}:${TACC_DARSHAN_LIB}"
    else
	IBRUN_LD_PRELOAD="${IBRUN_LD_PRELOAD}:${TACC_DARSHAN_FORTRAN_LIB}:${TACC_DARSHAN_LIB}"
    fi
fi

if [ -n "$IBRUN_LD_PRELOAD" ]; then
    MVAPICH_LD_PRELOAD="LD_PRELOAD=$IBRUN_LD_PRELOAD"
    OPENMPI_LD_PRELOAD="-x LD_PRELOAD=$IBRUN_LD_PRELOAD"
fi


if [ x"$MODE" == "xmvapich2_slurm" -o x"$MODE" == "xmvapich2_ssh" ]; then

    # Build hostfile for mvapich2 (with SLURM integeration) 
    
    if [ x"$MODE" == "xmvapich2_slurm" ];then
	echo "TACC: Setting up parallel environment for MVAPICH2+SLURM."
    elif [ x"$MODE" == "xmvapich2_ssh" ]; then
	echo "TACC: Setting up parallel environment for MVAPICH2+mpispawn."
    fi

    #Build hostfile using guidance from SLURM
    #First build the hostlist
    scontrol show hostname $SLURM_NODELIST > $hostfile_tacc".tmp"
    declare -a hostlist=(`scontrol show hostname $SLURM_NODELIST `) 

    if [ $? -ne 0  ];then
	echo "TACC: Error -> slurm host list unavailable"
	exit 1
    fi
#    echo "DEBUG: ${hostlist[@]} "

    #Initialize the hostlist index
    host_id=0
    #Set up the PE task index to store wayness for each task
    # this is for tacc_affinity
    task_id=0

    #Build the hostlist using the SLURM_TASKS_PER_NODE syntax
    for nodes in ${node_clusters[@]}; do
      #Get the task count and node count for each node cluster
      task_count=`echo $nodes | awk -F '(' '{print $1}'`
      if [[ `echo $nodes | grep x` ]]; then
        node_count=`echo $nodes | sed -e's/.*x\([0-9]\+\).*/\1/'`
      else
        node_count=1
      fi
#      echo "DEBUG: nodes=$nodes task_count=$task_count  node_count=$node_count"

      #Build the host list to match tasks per node
      # and set up PE env variable for each task
      for i in `seq 0 $((node_count-1))`; do
        for j in `seq 0 $((task_count-1))`; do
          echo ${hostlist[${host_id}]} >> $hostfile_tacc
          ((task_id++))
        done
        ((host_id++))
      done
    done

    # Default IB settings for MVAPICH2

    export MV2_DEFAULT_TIME_OUT=$default_time_out

    # Temporary settings to cooperate with MIC environment (ks 7/31/12)

    export MV2_IBA_HCA=mlx4_0
    export MV2_USE_RING_STARTUP=0

elif [ x"$MODE" == "ximpi_hydra" ]; then

    #Build hostfile using guidance from SLURM
    #First build the hostlist
    scontrol show hostname $SLURM_NODELIST > $hostfile_tacc".tmp"
    declare -a hostlist=(`scontrol show hostname $SLURM_NODELIST `) 

    if [ $? -ne 0  ];then
	echo "TACC: Error -> slurm host list unavailable"
	exit 1
    fi
#    echo "DEBUG: ${hostlist[@]} "

    #Initialize the hostlist index
    host_id=0
    #Build the hostlist using the SLURM_TASKS_PER_NODE syntax
    for nodes in ${node_clusters[@]}; do
      #Get the task count and node count for each node cluster
      task_count=`echo $nodes | awk -F '(' '{print $1}'`
      if [[ `echo $nodes | grep x` ]]; then
        node_count=`echo $nodes | sed -e's/.*x\([0-9]\+\).*/\1/'`
      else
        node_count=1
      fi
#      echo "DEBUG: nodes=$nodes task_count=$task_count  node_count=$node_count"

      #Build the host list to match tasks per node
      for i in `seq 0 $((node_count-1))`; do
        for j in `seq 0 $((task_count-1))`; do
          echo ${hostlist[${host_id}]} >> $hostfile_tacc
        done
        ((host_id++))
      done
    done

    rm -f $hostfile_tacc".tmp"

    # Let's assume host runs for now...clearly this must change when things start cooperating.

    if [ -z "$I_MPI_OFA_ADAPTER_NAME" ]; then
	export I_MPI_OFA_ADAPTER_NAME=mlx4_0
    fi

    if [ -z "$I_MPI_FABRICS" ]; then

    # export I_MPI_FABRICS=shm:ofa
    # Updated on 3/13/2013 to use DAPL

	export I_MPI_FABRICS=shm:dapl
    fi
    

else
    # Some other MPI stack? fail.

    echo "TACC: Could not determine MPI stack. Exiting!"
    exit 1
fi

#echo "TACC: Setup complete."

# ------------------------------
# Check for user provided NSLOTS
# ------------------------------

if [ x"$MY_NSLOTS" == "x" ]; then
    if [ -f $nslotsfile_tacc ]; then
	MY_NSLOTS=`cat $nslotsfile_tacc`
    else
	MY_NSLOTS=$NSLOTS_BATCH
    fi
fi

#------------------------------
# Let's finally launch the job
#------------------------------

# Check for some command line switches before the executable
stop_parsing=0

while [ $stop_parsing -ne 1 ]
do
    case "$1" in
	-np)
	    shift
	    MY_NSLOTS=$1
	    shift
	    ;;
	-n)
      ## This is how many hosts to use from the main hostfile.
      ## Can be as many as $MY_NSLOTS.
      # echo "Found -n switch to ibrun."
	    shift
      # echo "Argument of -n is $1"
	    ibrun_n_option=$1
	    shift
	    ;;
	
	-o)
      ## This is the offset into the host file where we begin
      ## to grab hosts.
      # echo "Found -o switch to ibrun."
	    shift
      # echo "Argument of -o is $1"
	    ibrun_o_option=$1
	    shift
	    ;;

	*)
      ## Default case
      # echo "Reached default case, we assume this is the executable..."
	    cmd=$1
	    shift
	    stop_parsing=1
	    ;;
    esac
done

## Do some error checking of the user's arguments to ibrun.
res=0

## If -n is set, must also set -o, and vice-versa.  This way we don't
## have to worry about cases where one is set and the other is not.
#
# Is -n set and -o not set?
if [ x"$ibrun_n_option" != "x" -a x"$ibrun_o_option" == "x" ]; then
    echo "ERROR: The -n option to ibrun was set but -o was not."
    res=1
fi

# Or, is -n unset while -o is?
if [ x"$ibrun_n_option" == "x" -a x"$ibrun_o_option" != "x" ]; then
    echo "ERROR: The -o option to ibrun was set but -n was not."
    res=1
fi

# Exit now if either of the preceding 2 tests failed.
if [ $res -ne 0 ]; then
    echo "TACC: MPI job exited with code: $res"
    exit 1
fi


## If set, check -n and -o options for non-numeric input.
if [ x"$ibrun_n_option" != "x" ]; then
    echo $ibrun_n_option | grep "[^0-9]" > /dev/null 2>&1
    if [ "$?" -eq "0" ]; then
	echo "ERROR: Non-numeric argument passed for -n."
	res=1
    fi
    
    echo $ibrun_o_option | grep "[^0-9]" > /dev/null 2>&1
    if [ "$?" -eq "0" ]; then
	echo "ERROR: Non-numeric argument passed for -o."
	res=1
    fi
fi

## Exit in case of non-numeric input.
if [ $res -ne 0 ]; then
    echo "TACC: MPI job exited with code: $res"
    exit 1
fi


if [ x"$ibrun_n_option" != "x" ]; then
    ## Double check user's -n value: it can't be larger than MY_NSLOTS
    if [ $ibrun_n_option -gt $MY_NSLOTS ]; then
	echo "ERROR: -n option requested $ibrun_n_option hosts, only $MY_NSLOTS are available!"
	res=1
    fi
    
    ## Double check user's -o value: it can't be larger than MY_NSLOTS
    if [ $ibrun_o_option -gt $MY_NSLOTS ]; then
	echo "ERROR: -o option requested $ibrun_o_option offset, only $MY_NSLOTS are available!"
	res=1
    fi

    ## Double check to see that the requested offset+the number of processors does not
    ## put us over the total number of hosts.  (Don't allow wrap-around.)
    slots_max=$(( $ibrun_o_option + $ibrun_n_option ))
    if [ $slots_max -gt $MY_NSLOTS ]; then
	echo "ERROR: Number of hosts (requested through -n) plus offset (via -o) exceeds the total number of slots available."
	res=1
    fi

    ## Offset of zero (-o 0) is OK but requesting 0 hosts (-n 0) doesn't really make sense...
    if [ $ibrun_n_option -eq 0 ]; then
	echo "ERROR: Requesting zero hosts (-n 0) is not allowed."
	res=1
    fi
fi

## Exit in case of invalid -n or -o option.
if [ $res -ne 0 ]; then
    echo "TACC: MPI job exited with code: $res"
    exit 1
fi

if [ -n "$LMOD_CMD" ]; then
    export LMOD_CMD=$LMOD_CMD
fi

# Double check the executable name specified to ibrun
fullcmd=`which $cmd 2>/dev/null`
if [ $? -ne 0 ]; then
#  echo "$cmd not found"
#  exit 1
    fullcmd="$cmd"
fi

if [ -n "$TACC_IBWRAPPER_DEBUG" ]; then
    exit
fi

eval "pe_mapD=\$ADMIN_$SYSHOST"
pe_fn=`mktemp $home_batch_dir/job.$BATCH_JOB_ID.usage.XXXXXXXX`
mv $pe_fn $pe_fn.lua
pe_fn=$pe_fn.lua

if [ -x "$ibrunDir/checkExec" ]; then
  $ibrunDir/checkExec --map $pe_mapD -- $fullcmd "$@"
fi

if [ -x "$ibrunDir/parseLDD" ]; then
  $ibrunDir/parseLDD --map $pe_mapD --start $pe_startTime --runTime 0.0 --fn $pe_fn -- $fullcmd "$@"
fi

XALT_DIR=@xalt_dir@
RUN_SUBMIT=$XALT_DIR/libexec/xalt_run_submission.py

RUN_UUID=`@path_to_uuidgen@`
DATESTR=`date +%Y_%m_%d_%H_%M_%S`
runFn=$HOME/.xalt.d/run.${SYSHOST}.${DATESTR}.$RUN_UUID.json
sTime=$($RUN_SUBMIT --start 0        --fn "$runFn" --run_uuid "$RUN_UUID" -- $fullcmd "$@")

## Modify $hostfile_tacc if user passed special options to ibrun

if [ x"$ibrun_n_option" != "x" ]; then

  # Handle ssh mvapich 
    if [ x"$MODE" == "xmvapich1_ssh" -o x"$MODE" == "xmvapich1_devel_ssh" -o x"$MODE" == "xmvapich2_ssh" -o x"$MODE" == "ximpi_hydra" ]; then
	if [ x"$ibrun_n_option" != x -a "x$MODE" == xmvapich1_ssh ]; then
	    rem=$(($ibrun_o_option % $CPN))
	    if [ "$rem" != 0 ]; then
		export VIADEV_USE_AFFINITY=0
		export VIADEV_ENABLE_AFFINITY=0
	    fi
	fi
	if [ x"$ibrun_n_option" != x -a "x$MODE" == xmvapich2_ssh ]; then
	    rem=$(($ibrun_o_option % $CPN))
	    if [ "$rem" != 0 ]; then
		export MV2_USE_AFFINITY=0
		export MV2_ENABLE_AFFINITY=0
	    fi
	fi

	if [ x"$ibrun_n_option" != x -a "x$MODE" == "ximpi_hydra" ]; then
	    rem=$(($ibrun_o_option % $CPN))
	    if [ "$rem" != 0 ]; then
                export I_MPI_PIN=0
	    fi
	fi

    # Create a temporary file for the subset of hosts which we will run on.
	subhostfile_tacc=`mktemp $home_batch_dir/job.$BATCH_JOB_ID.subhostlist.XXXXXXXX`

    # Cut the subset of hosts from the orig hostfile and put them in the subhostfile.
	cat $hostfile_tacc | tail -n $(( $MY_NSLOTS - $ibrun_o_option )) | head -n $ibrun_n_option > $subhostfile_tacc

  # Handle openmpi mpi starter
    elif [ x"$MODE" == "xopenmpi_ssh" -o x"$MODE" == "xopenmpi_1.3_ssh" ]; then
    # Create a temporary file for the subset of hosts which we will run on.
	subhostfile_tacc=`mktemp $home_batch_dir/job.$BATCH_JOB_ID.subhostlist.XXXXXXXX`

    #Calculate the starting node and ending node
	s_node=$(( $ibrun_o_option/$pe_ppn ))
	e_node=$(( ($ibrun_o_option+$ibrun_n_option)/$pe_ppn ))
	m_nodes=$(( $e_node - $s_node - 1 ))

    #Calculate the # of tasks on starting node and ending node
	s_tasks=$(( $pe_ppn - ($ibrun_o_option - ($s_node*$pe_ppn)) ))
	e_tasks=$(( $ibrun_o_option + $ibrun_n_option - ($e_node*$pe_ppn) ))
    # Cut the subset of hosts from the orig hostfile and put them in the subhostfile.

    #Handle the first node
	cat $hostfile_tacc | head -n $(( $s_node + 1 )) | tail -1 | sed -e"s/=[0-9]\+$/=${s_tasks}/" >  $subhostfile_tacc
    #Handle the in between nodes
	cat $hostfile_tacc | head -n $e_node   | tail -n $m_nodes  >>  $subhostfile_tacc
    #Handle the last node
	cat $hostfile_tacc | head -n $(( $e_node + 1 )) | tail -1 | sed -e"s/=[0-9]\+$/=${e_tasks}/" >> $subhostfile_tacc

    fi

  # Move the subhostfile in place of the hostfile
    /bin/mv $subhostfile_tacc $hostfile_tacc

  ## Set MY_NSLOTS to the number of requested processors so the mpirun commands below
  ## use the right number of CPUs.
    MY_NSLOTS=$ibrun_n_option
fi

echo "TACC: Starting parallel tasks..."


# Launch a job with mvapich2+MPD's mpiexec command
if [ x"$MODE" == "xmvapich2_mpd" ]; then

    mpiexec -machinefile $hostfile_tacc -np $MY_NSLOTS $fullcmd "$@"
    res=$?


elif [ x"$MODE" == "xmvapich2_ssh" ]; then

    # Launch a job with mvapich1 and 2+ssh mpirun_rsh command

    TACC_ENV=`build_env.pl`
#    echo "DEBUG: $MPICH_HOME/bin/mpirun_rsh -np $MY_NSLOTS -hostfile $hostfile_tacc $MY_MPIRUN_OPTIONS $TACC_ENV $MVAPICH_LD_PRELOAD $fullcmd $@ "
    $MPICH_HOME/bin/mpirun_rsh -np $MY_NSLOTS -hostfile $hostfile_tacc $MY_MPIRUN_OPTIONS $TACC_ENV $MVAPICH_LD_PRELOAD $fullcmd "$@"
    res=$?

elif [ x"$MODE" == "ximpi_hydra" ]; then

    $MPICH_HOME/intel64/bin/mpiexec.hydra -np $MY_NSLOTS -machinefile $hostfile_tacc $MY_MPIRUN_OPTIONS $fullcmd "$@"
    res=$?

elif [ x"$MODE" == "xmvapich2_slurm" ]; then

    srun -n $MY_NSLOTS $MY_MPIRUN_OPTIONS $MVAPICH_LD_PRELOAD $fullcmd "$@"
    res=$?

elif [ x"$MODE" == "xopenmpi_ssh" -o  x"$MODE" == "xopenmpi_1.3_ssh" ]; then

    TACC_ENV=`env | cut -d= -f1 | grep -v '\b_\b' | perl -pe 's/(.*)\n/ -x \1/m'`
    TACC_OPENMPI_OPTIONS="--mca btl sm,openib,self --mca btl_openib_ib_timeout $default_time_out --mca btl_openib_use_srq 1 --mca btl_openib_use_rd_max $srq_size"

    mpirun -np $MY_NSLOTS -hostfile $hostfile_tacc --mca oob_tcp_if_include ib0 $TACC_OPENMPI_OPTIONS $MY_OPENMPI_OPTIONS --prefix $MPICH_HOME $TACC_ENV $OPENMPI_LD_PRELOAD $fullcmd "$@"
    res=$?


# The mode was not correctly set, set fail.  Probably can't get here
# because we would have already failed during machinefile setting time.
else
    echo -e "TACC: Could not determine which MPI stack to use.\nTACC:Exiting.\n"
    res=1
fi


pe_endTime=`date +%s`
pe_runTime=`echo "$pe_endTime - $pe_startTime" | bc -q`

if [ -x "$ibrunDir/parseLDD" ]; then
  $ibrunDir/parseLDD --map $pe_mapD --start $pe_startTime --runTime $pe_runTime --fn $pe_fn -- $fullcmd "$@"
fi

rTime=$($RUN_SUBMIT --start "$sTime" --fn "$runFn" --run_uuid "$RUN_UUID" -- $fullcmd "$@")


if [ $res -ne 0 ]; then
    echo "TACC: MPI job exited with code: $res"
fi


#-----------------------
# Job tear-down/cleanup
#-----------------------

###echo "TACC: Shutting down parallel environment."
###
###if [ x"$MODE" == "xmvapich2_mpd" ]; then
###    mpdallexit 
###elif [ x"$MODE" == "xmvapich1_ssh" -o x"$MODE" == "xmvapich2_ssh" ]; then
###    /bin/true
###elif [ x"$MODE" == "xmvapich1_devel_ssh" ]; then
###    /bin/true
###elif [ x"$MODE" == "xopenmpi_ssh" -o x"$MODE" == "xopenmpi_1.3_ssh" ]; then
###    /bin/true
###else
###    echo "TACC: You should not see this message! Please contact TACC consulting and send us your jobscript."
###    exit 1
###fi

if [ x"$TACC_KEEP_FILES" != "x" ]; then
    if [ -f $nslotsfile_tacc ]; then
	rm $nslotsfile_tacc
    fi
    if [ -f $hostfile_tacc ]; then
	rm $hostfile_tacc
    fi
fi

echo " "
echo "TACC: Shutdown complete. Exiting." 
exit $res



